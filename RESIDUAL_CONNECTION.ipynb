{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**RESIDUAL  CONNECTION"
      ],
      "metadata": {
        "id": "CZGP985RwHeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A residual connection bypasses one or more layers and adds the input directly to the output of a deeper layer.\n",
        "\n",
        "Mathematically, if a function\n",
        "𝐹\n",
        "(\n",
        "𝑥\n",
        ")\n",
        "F(x) represents the transformation done by a few layers, the residual connection computes:\n",
        "\n",
        "𝑌\n",
        "=\n",
        "𝐹\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "+\n",
        "𝑋\n",
        "Y=F(X)+X\n",
        "where:\n",
        "\n",
        "𝑋\n",
        "X = Input to the residual block\n",
        "𝐹\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "F(X) = Transformation by intermediate layers (e.g., convolution, activation, etc.)\n",
        "𝑌\n",
        "Y = Final output after adding residual connection\n",
        "Instead of learning the function\n",
        "𝐻\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "H(X) directly, the network learns a residual function:\n",
        "\n",
        "𝐹\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "=\n",
        "𝐻\n",
        "(\n",
        "𝑋\n",
        ")\n",
        "−\n",
        "𝑋\n",
        "F(X)=H(X)−X\n",
        "This makes it easier for the network to optimize."
      ],
      "metadata": {
        "id": "_xkRyvLwv0kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jkPkmkUWwDkC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "VIj5VaKawBHi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "   def init_(self,dropout:float) -> None:\n",
        "    super() ._init_()\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.norm=LayerNormalization()\n",
        "\n",
        "   def forward(self,x,sublayer):\n",
        "    #return x + self.dropout(self.norm(sublayer(x)))\n",
        "    return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "WY1U_FWrwQSN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uQh58x7Twsjs"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}