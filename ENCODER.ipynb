{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        ""
      ],
      "metadata": {
        "id": "fo-4l7gkolwm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Encoder Block\n",
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        # Storing the self-attention block and feed-forward block\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # Applying the first residual connection with the self-attention block\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # Three 'x's corresponding to query, key, and value inputs plus source mask\n",
        "\n",
        "        # Applying the second residual connection with the feed-forward block\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x # Output tensor after applying self-attention and feed-forward layers with residual connections.\n"
      ],
      "metadata": {
        "id": "YOUPQFUPoGWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Building Encoder\n",
        "# An Encoder can have several Encoder Blocks\n",
        "class Encoder(nn.Module):\n",
        "\n",
        "    # The Encoder takes in instances of 'EncoderBlock'\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers # Storing the EncoderBlocks\n",
        "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        # Iterating over each EncoderBlock stored in self.layers\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n",
        "        return self.norm(x) # Normalizing output\n"
      ],
      "metadata": {
        "id": "KbzBnodnohy5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}